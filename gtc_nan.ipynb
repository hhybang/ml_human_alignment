{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbe6312-dd91-4a70-9415-dc5349ac1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbcb2f0-453a-4194-973f-f49f967445d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL.Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "from statistics import mean\n",
    "import random\n",
    "import gc\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from shared_interest.datasets.imagenet import ImageNet\n",
    "from shared_interest.shared_interest import shared_interest\n",
    "from shared_interest.util import flatten, normalize_0to1, binarize_std, binarize_percentile\n",
    "from interpretability_methods.vanilla_gradients import VanillaGradients\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a94879aa-2fdc-4871-a4a9-2941c196bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d93b64ec-bdd7-4007-8e54-4303df5ebda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieved from Shared Interest Repository (edited for debugging purposes) \n",
    "\n",
    "\"\"\"Dataset for ImageNet with annotations.\"\"\"\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "class ImageNet(ImageFolder):\n",
    "    \"\"\"Extends ImageFolder dataset to include ground truth annotations.\"\"\"\n",
    "\n",
    "    def __init__(self, image_path, ground_truth_path, image_transform=None,\n",
    "                 ground_truth_transform=None):\n",
    "        \"\"\"\n",
    "        Extends the parent class with annotation information.\n",
    "\n",
    "        Additional Args:\n",
    "        image_path: the path to the ImageNet images. This folder must be\n",
    "            formatted in ImageFolder style (i.e. label/imagename.jpeg)\n",
    "        ground_truth_path: the path to the ImageNet annotations. This folder\n",
    "            must be formated in ImageFolder style (i.e., label/imagename.xml).\n",
    "        image_transform: a pytorch transform to apply to the images or None.\n",
    "            Defaults to None.\n",
    "        ground_truth_transform: a pytorch transform to apply to the ground\n",
    "            truth annotations or None. Defaults to None.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__(image_path, transform=image_transform)\n",
    "        self.ground_truth_transform = ground_truth_transform\n",
    "        self.ground_truth_path = ground_truth_path\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns the image, ground_truth mask, and label of the image.\"\"\"\n",
    "        image, _ = super().__getitem__(index)\n",
    "        image_path, _ = self.imgs[index]\n",
    "        image_name = image_path.strip().split('/')[-1].split('.')[0]\n",
    "        label = image_path.strip().split('/')[-2]\n",
    "\n",
    "        ground_truth_file = os.path.join(self.ground_truth_path, label, '%s.xml' %image_name)\n",
    "        ground_truth = self._create_ground_truth(ground_truth_file)\n",
    "        # if (not torch.any(ground_truth)):\n",
    "        #     print (\"BEFORE all zeros: \" + str(index) + \" (\" + ground_truth_file + \")\")\n",
    "        # ground_truth_before = ground_truth.clone().detach()\n",
    "        zero_ground = 0 \n",
    "        if self.ground_truth_transform is not None:\n",
    "            ground_truth = self.ground_truth_transform(ground_truth).squeeze(0)\n",
    "        if (not torch.any(ground_truth)):\n",
    "            zero_ground = index\n",
    "            # print (\"item index: \" + str(index))\n",
    "            # print (str(index) + \" (\" + ground_truth_file + \")\")\n",
    "        return image_path, ground_truth_file, image, ground_truth, int(label), zero_ground\n",
    "\n",
    "    def _create_ground_truth(self, ground_truth_file):\n",
    "        \"\"\"Creates a binary groudn truth mask based on the ImageNet annotations.\"\"\"\n",
    "        annotation = self._parse_xml(ground_truth_file)\n",
    "        height, width = int(annotation['height']), int(annotation['width'])\n",
    "        ground_truth = torch.zeros((height, width))\n",
    "        for coordinate in annotation['coordinates']:\n",
    "            y_min, y_max = int(coordinate['ymin']), int(coordinate['ymax'])\n",
    "            x_min, x_max = int(coordinate['xmin']), int(coordinate['xmax'])\n",
    "            ground_truth[y_min:y_max, x_min:x_max] = 1\n",
    "        # if (ground_truth_file == '/nobackup/users/hbang/data/imagenet/val_old/annotations/0109/ILSVRC2012_val_00016569.xml'):\n",
    "        #     print (annotation)\n",
    "        #     print (torch.count_nonzero(ground_truth))\n",
    "        return ground_truth\n",
    "\n",
    "    def _parse_xml(self, ground_truth_file):\n",
    "        \"\"\"Parse ImageNet annotation XML file.\"\"\"\n",
    "        if not os.path.isfile(ground_truth_file):\n",
    "            raise IOError('No annotation data for %s.' %(ground_truth_file))\n",
    "        tree = ET.parse(ground_truth_file)\n",
    "        root = tree.getroot()\n",
    "        bboxes = [obj.find('bndbox') for obj in root.findall('object')]\n",
    "        coords = [{'xmin': int(bbox.find('xmin').text),\n",
    "                   'ymin': int(bbox.find('ymin').text),\n",
    "                   'xmax': int(bbox.find('xmax').text),\n",
    "                   'ymax': int(bbox.find('ymax').text), } for bbox in bboxes]\n",
    "        height = root.find('size').find('height').text\n",
    "        width = root.find('size').find('width').text\n",
    "        return {'coordinates': coords, 'height': height, 'width': width}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc961133-e072-4240-8361-8a9ae971d815",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_dir = '/nobackup/users/hbang/data/imagenet/val_old/'\n",
    "image_dir = os.path.join(imagenet_dir, 'images')\n",
    "annotation_dir = os.path.join(imagenet_dir, 'annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45789f91-d700-468c-a453-767f5372a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet transforms.\n",
    "image_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                                           std=[0.229, 0.224, 0.225]),\n",
    "                                     ])\n",
    "\n",
    "ground_truth_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                             transforms.Resize(256, PIL.Image.NEAREST),\n",
    "                                             transforms.CenterCrop(224),\n",
    "                                             transforms.ToTensor()])\n",
    "\n",
    "reverse_image_transform = transforms.Compose([transforms.Normalize(mean=[0, 0, 0], \n",
    "                                                                   std=[4.3668, 4.4643, 4.4444]),\n",
    "                                              transforms.Normalize(mean=[-0.485, -0.456, -0.406], \n",
    "                                                                   std=[1, 1, 1]),\n",
    "                                              transforms.ToPILImage(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4300f2ac-8b35-43bd-9ebd-5fcb47211599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(image_dir, annotation_dir, image_transform, ground_truth_transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, \n",
    "                                         num_workers=10, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "699ab510-b87b-4cb4-9b61-9e08dce2f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml(ground_truth_file):\n",
    "    \"\"\"Parse ImageNet annotation XML file.\"\"\"\n",
    "    if not os.path.isfile(ground_truth_file):\n",
    "        raise IOError('No annotation data for %s.' %(ground_truth_file))\n",
    "    tree = ET.parse(ground_truth_file)\n",
    "    root = tree.getroot()\n",
    "    bboxes = [obj.find('bndbox') for obj in root.findall('object')]\n",
    "    coords = [{'xmin': int(bbox.find('xmin').text),\n",
    "               'ymin': int(bbox.find('ymin').text),\n",
    "               'xmax': int(bbox.find('xmax').text),\n",
    "               'ymax': int(bbox.find('ymax').text), } for bbox in bboxes]\n",
    "    height = root.find('size').find('height').text\n",
    "    width = root.find('size').find('width').text\n",
    "    return {'coordinates': coords, 'height': height, 'width': width}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8e8ef-7a6b-4007-98d3-660997469290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking what is causing the 0 ground_truth values \n",
    "\n",
    "torch.set_printoptions(threshold=900_000)\n",
    "model = timm.create_model(timm.list_models('*coatnet*')[-1], pretrained=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "saliency_method =  VanillaGradients(model)\n",
    "model.eval()\n",
    "\n",
    "# nan_ground_truth = {}\n",
    "\n",
    "for i, (image_path, ground_truth_file, images, ground_truth, labels, zero_ground) in enumerate(tqdm(dataloader, position=0, leave=True)):\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        ground_truth = ground_truth.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        # deleting 0 ground truths from dataset \n",
    "        # if i in nan_ground_truth:\n",
    "        #     nans = nan_ground_truth[i]\n",
    "        #     for j in sorted(nans, reverse=True):\n",
    "        #         images = torch.cat([images[0:j], images[j+1:]])\n",
    "        #     ground_truth = np.delete(ground_truth, nans, 0)\n",
    "        #     labels = np.delete(labels, nans, 0)\n",
    "            \n",
    "        # nonzero_ground = torch.nonzero(zero_ground, as_tuple = True)[0].tolist()\n",
    "        # if (len(nonzero_ground) > 0):\n",
    "        #     nan_ground_truth[i] = nonzero_ground \n",
    "\n",
    "        \n",
    "        # Compute Shared Interest scores\n",
    "        for score in total_shared_interest_scores:\n",
    "            shared_interest_scores = shared_interest(ground_truth, saliency_masks, score=score)\n",
    "            if score == 'ground_truth_coverage' and np.isnan(shared_interest_scores).any():\n",
    "                index = sum(np.argwhere(np.isnan(shared_interest_scores)).tolist(), [])\n",
    "                # check if the set of indices of GTC score of NaN and set of indicies of ground_truth of 0 are equal \n",
    "                if not (set(index) == set(nonzero_ground)):\n",
    "                    print (\"not equal\")\n",
    "                    print (index) \n",
    "                    print (nonzero_ground)\n",
    "                    print (\"-------------\")\n",
    "                \n",
    "                # check if for each annotation, the 0 is happening due to annotation being scaled to 224 \n",
    "                for j in index: \n",
    "                    annotation = parse_xml(ground_truth_file[j])\n",
    "                    # print (annotation)\n",
    "\n",
    "                    x_ = int(annotation['width'])\n",
    "                    y_ = int(annotation['height'])\n",
    "                    if (x_ < y_):\n",
    "                        scale = 256 / x_\n",
    "                        x = 256\n",
    "                        y = int(y_ * (scale))\n",
    "                    else: \n",
    "                        scale = 256 / y_\n",
    "                        y = 256\n",
    "                        x = int(x_ * (scale))\n",
    "\n",
    "                    for coordinate in annotation['coordinates']:\n",
    "                        xmin = int(np.round(int(coordinate['xmin'])) * scale)\n",
    "                        ymin = int(np.round(int(coordinate['ymin'])) * scale)\n",
    "                        xmax = int(np.round(int(coordinate['xmax'])) * scale)\n",
    "                        ymax = int(np.round(int(coordinate['ymax'])) * scale)\n",
    "                        \n",
    "                        xleft = xmax <= ((x - 224) / 2)\n",
    "                        xright = xmin >= ((x + 224) / 2)\n",
    "                        yup = ymax <= ((y - 224) / 2)\n",
    "                        ydown = ymin >= ((y + 224) / 2)\n",
    "                        \n",
    "                        # coming from the pixels being inbetween \n",
    "                        err_xleft = (xmax - ((x - 224) / 2)) < 2\n",
    "                        err_xright = (xmin - ((x + 224) / 2)) > -2\n",
    "                        err_yup = (ymax - ((y - 224) / 2)) < 2\n",
    "                        err_ydown = (ymin - ((y + 224) / 2)) > -2\n",
    "                        if (xleft or xright or yup or ydown):\n",
    "                            pass\n",
    "                        elif (err_xleft or err_xright or err_yup or err_ydown):\n",
    "                            print (x,y)\n",
    "                            print (annotation)\n",
    "                            print (coordinate)\n",
    "                            print (xmin, xmax)\n",
    "                            print (ymin, ymax)\n",
    "                            print ((xmax - ((x - 224) / 2)), (xmin - ((x + 224) / 2)))\n",
    "                            print ((ymax - ((y - 224) / 2)), (ymin - ((y + 224) / 2)))\n",
    "                            print (\"----------------\")\n",
    "                        else :\n",
    "                            print (\"not wrong?\")\n",
    "                            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bc26407-fcd9-4b5f-bddd-2dcc8a3867dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "NAN\n",
      "[0.21652526 0.17744452 0.11775148 ... 0.23425196 0.17495775 0.13681567]\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.abspath(\"/home/hbang/data/vanilla_gradient/\" + timm.list_models('*coatnet*')[-1] + \"/\")\n",
    "with open(os.path.join(model_path, \"shared_interest_scores.pickle\"), 'rb') as handle:\n",
    "    shared_interest_scores = pickle.load(handle)\n",
    "\n",
    "gtc_coverage = shared_interest_scores['ground_truth_coverage']\n",
    "print (gtc_coverage.size)\n",
    "if np.isnan(gtc_coverage).any():\n",
    "    print (\"NAN\")\n",
    "print (gtc_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "997381c3-17ab-4a1c-afb4-88bf4cfd8be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5463  7509 15345 16151 16194 20090 20265 20273 20276 20295 20297 20323\n",
      " 20637 20800 20803 20804 20806 20808 20809 20814 20820 20821 20825 20835\n",
      " 20839 20842 20845 20859 20894 20986 21360 21379 21451 21469 21473 21481\n",
      " 21482 21494 21501 21504 21510 21518 21520 21521 21522 21535 21542 21548\n",
      " 21549 21618 21673 21694 21860 21866 21875 21965 22754 22897 23156 23242\n",
      " 24487 24971 25492 25720 25729 25753 26105 26114 26118 26136 26550 26559\n",
      " 26812 26818 26829 27852 27891 27929 28488 29075 29250 29274 30107 30111\n",
      " 30133 30135 30138 30141 30145 30148 30587 30765 31579 32114 32138 32557\n",
      " 32765 33352 33364 33876 34010 34131 34695 34987 35125 35441 35758 35762\n",
      " 35763 35764 35772 35773 35776 35777 35781 35782 35785 35786 35787 35789\n",
      " 35793 35794 35796 35797 36119 36584 36698 37103 37180 37213 37314 37316\n",
      " 37318 37323 37335 37336 37340 37342 37347 37348 37349 37804 38357 38432\n",
      " 38441 38777 39263 39680 39697 40280 40939 40941 40944 41617 42043 42149\n",
      " 42296 42617 42620 42622 42626 42631 43107 44504 44514 44519 44521 44528\n",
      " 44533 44534 44536 44537 44538 44539 44545 44548 45341 46027 46047 46841\n",
      " 47297 47384 47533 47584 49389]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "nans = np.array([])\n",
    "for i in range (0, gtc_coverage.size):\n",
    "    if np.isnan(gtc_coverage[i]):\n",
    "        n += 1\n",
    "        nans = np.append(nans, i)\n",
    "# print (nans) # total 185 of NANs in GTC\n",
    "# print (nans.size)\n",
    "nans = nans.astype(int)\n",
    "print (nans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
